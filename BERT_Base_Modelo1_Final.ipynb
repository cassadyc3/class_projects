{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bCUQ-fchiaym",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "a80efa84-b57a-4900-e157-7f8e3bdcde6e",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 30.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 12.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Collecting dill<0.3.5\n",
            "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.0 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.7.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 57.2 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 34.7 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, dill, aiohttp, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.13\n",
            "    Uninstalling multiprocess-0.70.13:\n",
            "      Successfully uninstalled multiprocess-0.70.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 multiprocess-0.70.12.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "#!pip install src"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "arBRBKaaD4Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/Shareddrives/NLP/data_collection.py"
      ],
      "metadata": {
        "id": "LpV_3CwGW_yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3oE1mStW3fl"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "sys.path.insert(0, '..')\n",
        "#from data_collection import get_data\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datasets\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_data(dataset_name=\"ucberkeley-dlab/measuring-hate-speech\", columns=[\"text\", \"hatespeech\"]):\n",
        "    \"\"\"\n",
        "    Helper method which fetches the requested dataset, narrows it down to the\n",
        "    relevant columns, aggregates second column to the most frequent value\n",
        "    based on the first column, and returns it\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset_name : str, optional\n",
        "        Name of the dataset to be downloaded. For this project, the default\n",
        "        value is \"ucberkeley-dlab/measuring-hate-speech\".\n",
        "    columns : list, optional\n",
        "        A list of columns to be extracted. For this project, the default value\n",
        "        is  [\"text\", \"hatespeech\"].\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data : pandas.DataFrame\n",
        "        The fetched and processed dataset.\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"Fetching data...\")\n",
        "    dataset = datasets.load_dataset(dataset_name, \"binary\")\n",
        "    data = dataset[\"train\"].to_pandas()[columns]\n",
        "\n",
        "    print(\"Processing...\")\n",
        "    data[columns[1]] = pd.to_numeric(\n",
        "        data[columns[1]],\n",
        "        downcast=\"integer\"\n",
        "    )\n",
        "\n",
        "    data.loc[data[columns[1]] == 2, columns[1]] = 1\n",
        "\n",
        "    data = data.groupby(columns[0]).agg(\n",
        "        lambda x: Counter(x).most_common(1)[0][0]\n",
        "    ).reset_index()\n",
        "\n",
        "    print(\"Done!\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    BAD_WORDS = {\n",
        "        \"nigga\": \"n***a\",\n",
        "        \"fuck\": \"f**k\",\n",
        "        \"bitch\": \"b***h\",\n",
        "        \"dick\": \"d**k\",\n",
        "        \"cock\": \"c**k\",\n",
        "        \"ass\": \"a**\",\n",
        "        \"pussy\": \"p***y\",\n",
        "        \"sex\": \"s**\",\n",
        "        \"nigger\": \"n****r\",\n",
        "        \"faggot\": \"f****t\",\n",
        "        \"slut\": \"s**t\",\n",
        "        \"shit\": \"s**t\",\n",
        "        \"retard\": \"r****d\",\n",
        "        \"killed\": \"k****d\",\n",
        "        \"suck\": \"s**k\",\n",
        "        \"hoe\": \"h**\",\n",
        "        \"ugly\": \"u**y\",\n",
        "        \"nazi\": \"n**i\",\n",
        "        \"cunt\": \"c**t\",\n",
        "        \"cum\": \"c**\"\n",
        "    }\n",
        "    text = text.lower()\n",
        "    for word, replacement in BAD_WORDS.items():\n",
        "        text = text.replace(word, replacement)\n",
        "    return text"
      ],
      "metadata": {
        "id": "eXNaXybo3cFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EbRWhozrJHx"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xi2U8Gzq61C"
      },
      "outputs": [],
      "source": [
        "dataset = get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZwmaRkjZBNB"
      },
      "outputs": [],
      "source": [
        "print(len(dataset))\n",
        "print(dataset[\"text\"])\n",
        "print(dataset[\"hatespeech\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataset)"
      ],
      "metadata": {
        "id": "wcEYIQjGkNDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA10IEN4rp5-"
      },
      "outputs": [],
      "source": [
        "dataset[\"hatespeech\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe5WU3QxrBHG"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvDs3XoyrPIq"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"bert-base-uncased\"  \n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-05\n",
        "TOKENIZER = BertTokenizer.from_pretrained(MODEL_NAME, truncation=True, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EPWnO_E1IMF"
      },
      "outputs": [],
      "source": [
        "class Dataset_Preprocess(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = OneHotEncoder(sparse=False).fit_transform(np.array(self.data[\"hatespeech\"]).reshape(-1, 1))\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"targets\": torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG_xeoqi1JxX"
      },
      "outputs": [],
      "source": [
        "# Dataloader creation for dataset \n",
        "\n",
        "train_size = 0.8\n",
        "val_size = 0.1\n",
        "\n",
        "train_data = dataset.sample(frac = train_size)\n",
        "test_data = dataset.drop(train_data.index).reset_index(drop=True)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "val_data = test_data.sample(frac=val_size / (1 - train_size), random_state=220).reset_index()\n",
        "test_data = test_data.drop(val_data.index).reset_index(drop=True)\n",
        "\n",
        "print(f\"Full Dataset Size: {dataset.shape}\")\n",
        "print(f\"Train Dataset Size: {train_data.shape}\")\n",
        "print(f\"Validation Dataset Size: {val_data.shape}\")\n",
        "print(f\"Test Dataset Size: {test_data.shape}\")\n",
        "\n",
        "training_set = Dataset_Preprocess(train_data, TOKENIZER, MAX_LEN)\n",
        "validation_set = Dataset_Preprocess(val_data, TOKENIZER, MAX_LEN)\n",
        "testing_set = Dataset_Preprocess(test_data, TOKENIZER, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dtypes\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "qN6yqP_Akx-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lRXve2M1RC1"
      },
      "outputs": [],
      "source": [
        "train_params = {\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"shuffle\": True,\n",
        "    \"num_workers\": 0\n",
        "}\n",
        "\n",
        "val_params = {\n",
        "    \"batch_size\": 1,\n",
        "    \"shuffle\": False,\n",
        "    \"num_workers\": 0\n",
        "}\n",
        "\n",
        "test_params = {\n",
        "    \"batch_size\": 1,\n",
        "    \"shuffle\": False,\n",
        "    \"num_workers\": 0\n",
        "}\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "validation_loader = DataLoader(validation_set, **val_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7BYqeMCgOA0"
      },
      "source": [
        "## BERT Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4Y1wqFqcw_3"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import AutoModel\n",
        "from transformers import BertModel\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVgjGcPqiOCT"
      },
      "outputs": [],
      "source": [
        "class BERT_Base(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BERT_Base, self).__init__()\n",
        "        self.l1 = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.pre_classifier = nn.Linear(768, 768)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(768, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc36MMeUSrhd"
      },
      "outputs": [],
      "source": [
        "# class BERT_CNN(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(BERT_CNN, self).__init__()\n",
        "#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "#         self.conv = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 768), padding='valid')\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=(3,1), stride=1)\n",
        "#         self.dropout = nn.Dropout(0.1)\n",
        "#         self.fc = nn.Linear(416, 3)\n",
        "#         self.flat = nn.Flatten()\n",
        "#         self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "#     def forward(self, sent_id, mask, token_type_ids):\n",
        "#         _, all_layers = self.bert(input_ids = sent_id, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
        "#         # all_layers  = [13, 32, 64, 768]\n",
        "#         print('all layers', all_layers)\n",
        "#         print('all layers', all_layers.shape)\n",
        "#         x = torch.transpose(torch.cat(tuple([t.unsqueeze(0) for t in all_layers]), 0), 0, 1)\n",
        "#         del all_layers\n",
        "#         gc.collect()\n",
        "#         torch.cuda.empty_cache()\n",
        "#         print('Before dropout',x.shape)\n",
        "#         x = self.dropout(x)\n",
        "#         print('After dropout', x.shape)\n",
        "#         x = self.conv(x)\n",
        "#         print('After Convolutional layer', x.shape)\n",
        "#         x = self.pool(self.dropout(self.relu(x)))\n",
        "#         x = self.fc(self.dropout(self.flat(self.dropout(x))))\n",
        "#         return self.softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UwZCvwIF1wn0",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "num_classes = dataset[\"hatespeech\"].nunique()\n",
        "model = BERT_Base(n_classes = num_classes)\n",
        "model.to(device)\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNv8QeajDK3w"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFJ4LbFB2Ddz"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjSQpm-M2EhR"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KJjVyKA2G_x"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _, data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data[\"ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"mask\"].to(device, dtype=torch.long)\n",
        "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "        targets = data[\"targets\"].to(device, dtype=torch.float)\n",
        "        # print('ids', type(ids))\n",
        "        # print('mask', type(mask))\n",
        "        # print('token type ids', type(token_type_ids))\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if _ % 1000 == 0:\n",
        "            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTxX9PZa2JLb"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9-UsP-HDK3x"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzDPis2u2Nma"
      },
      "outputs": [],
      "source": [
        "def validation(model, loader):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(loader, 0)):\n",
        "            ids = data[\"ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"mask\"].to(device, dtype=torch.long)\n",
        "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "            targets = data[\"targets\"].to(device, dtype=torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6JRW7zL2Ogf"
      },
      "outputs": [],
      "source": [
        "outputs, targets = validation(model, validation_loader)\n",
        "\n",
        "final_outputs = np.argmax(outputs, axis=1)\n",
        "targets = np.argmax(targets, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsfAK0cM2Q4b"
      },
      "outputs": [],
      "source": [
        "print(f\"Got {sum(final_outputs == targets)} / {len(final_outputs)} correct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "242BUqcL2TjX"
      },
      "outputs": [],
      "source": [
        "micro_f1 = f1_score(targets, final_outputs, average=\"micro\")\n",
        "macro_f1 = f1_score(targets, final_outputs, average=\"macro\")\n",
        "weighted_f1 = f1_score(targets, final_outputs, average=\"weighted\")\n",
        "\n",
        "print(f\"Micro F1 score:\\t\\t{round(micro_f1, 3)}\")\n",
        "print(f\"Macro F1 score:\\t\\t{round(macro_f1, 3)}\")\n",
        "print(f\"Weighted F1 score:\\t{round(weighted_f1, 3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZaP5kNk2Wiz"
      },
      "outputs": [],
      "source": [
        "print(classification_report(targets, final_outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC0f54Hz2ZPR"
      },
      "outputs": [],
      "source": [
        "output_model_file = \"../content/drive/Shareddrives/NLP/pytorch_bert_cnn.bin\"\n",
        "\n",
        "output_vocab_file = \"../content/drive/Shareddrives/NLP/vocab_bert_cnn.bin\"\n",
        "\n",
        "torch.save(model, output_model_file)\n",
        "TOKENIZER.save_vocabulary(output_vocab_file)\n",
        "\n",
        "print(\"Model Saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application using YouTube videos"
      ],
      "metadata": {
        "id": "a7fDi_oKJwEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2G-2ShHyJnXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from csv import writer\n",
        "from apiclient.discovery import build\n",
        "import pickle\n",
        "import urllib.request\n",
        "import urllib"
      ],
      "metadata": {
        "id": "o8mhZXrIKqZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = 'AIzaSyA7IAgb20S12ZVAmvrsR6GVDY6iz-pVLxA' #replace with your youtube data api key\n",
        "#videoId = 'eYndEoy5Vr8' # This is embedded in the URL after \"v=\"\n",
        "\n",
        "videoId = 'ubKmjE3lEHI'\n",
        "# channelId = 'UC2UXDak6o7rBm23k3Vv5dww' "
      ],
      "metadata": {
        "id": "QcGxRlNUKvgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_service():\n",
        "    YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "    YOUTUBE_API_VERSION = \"v3\"\n",
        "    return build(YOUTUBE_API_SERVICE_NAME,\n",
        "                 YOUTUBE_API_VERSION,\n",
        "                 developerKey=key)"
      ],
      "metadata": {
        "id": "ol0KgVRsKyOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40846fdc-d6a6-4730-81ff-0ec57f2a26c5"
      },
      "outputs": [],
      "source": [
        "def get_comments(part='snippet', \n",
        "                 maxResults=100, \n",
        "                 textFormat='plainText',\n",
        "                 order='time',\n",
        "                 videoId=videoId,\n",
        "                 csv_filename=\"google2021search\"):\n",
        "\n",
        "    #3 create empty lists to store desired information\n",
        "    comments, commentsId, repliesCount, likesCount, viewerRating = [], [], [], [], []\n",
        "       \n",
        "    # build our service from path/to/apikey\n",
        "    service = build_service()\n",
        "    \n",
        "    #4 make an API call using our service\n",
        "    response = service.commentThreads().list(\n",
        "        part=part,\n",
        "        maxResults=maxResults,\n",
        "        textFormat=textFormat,\n",
        "        order=order,\n",
        "        videoId=videoId\n",
        "    ).execute()\n",
        "                 \n",
        "\n",
        "    while response: # this loop will continue to run until you max out your quota\n",
        "                 \n",
        "        for item in response['items']:\n",
        "            #5 index item for desired data features\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comment_id = item['snippet']['topLevelComment']['id']\n",
        "            reply_count = item['snippet']['totalReplyCount']\n",
        "            like_count = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
        "            \n",
        "            #6 append to lists\n",
        "            comments.append(comment)\n",
        "            commentsId.append(comment_id)\n",
        "            repliesCount.append(reply_count)\n",
        "            likesCount.append(like_count)\n",
        "\n",
        "            #7 write line by line\n",
        "            with open(f'{csv_filename}.csv', 'a+') as f:\n",
        "                # https://thispointer.com/python-how-to-append-a-new-row-to-an-existing-csv-file/#:~:text=Open%20our%20csv%20file%20in,in%20the%20associated%20csv%20file\n",
        "                csv_writer = writer(f)\n",
        "                csv_writer.writerow([comment, comment_id, reply_count, like_count])\n",
        "        \n",
        "        #8 check for nextPageToken, and if it exists, set response equal to the JSON response\n",
        "        if 'nextPageToken' in response:\n",
        "            response = service.commentThreads().list(\n",
        "                part=part,\n",
        "                maxResults=maxResults,\n",
        "                textFormat=textFormat,\n",
        "                order=order,\n",
        "                videoId=videoId,\n",
        "                pageToken=response['nextPageToken']\n",
        "            ).execute()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    #9 return our data of interest\n",
        "    return {\n",
        "        'Comments': comments,\n",
        "        'Comment ID': commentsId,\n",
        "        'Reply Count' : repliesCount,\n",
        "        'Like Count' : likesCount\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f1b7aa8-6fe5-4deb-8ca9-ff51b75ad4f3"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    google2021 = get_comments()\n",
        "    #df = pd.DataFrame(google2021)\n",
        "   # print(df.shape)\n",
        "    #print(df.head())\n",
        "    #df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    #df['just_date'] = df['date'].dt.date\n",
        "   # df.to_csv('./googlecomment.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b039c5c3-183e-4b54-ad0d-aac9328c1a9d"
      },
      "outputs": [],
      "source": [
        "with open(\"googlereview.json\", \"w\") as fh:\n",
        "    json.dump(google2021, fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e12e5b97-d166-4d96-908c-e31a74c76eb5"
      },
      "outputs": [],
      "source": [
        "comments = pd.read_json('googlereview.json')\n",
        "comments.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b04d51f-ae42-49f6-b97d-b1f2e027f4dc"
      },
      "outputs": [],
      "source": [
        "len(comments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "896a29df-60db-4a8a-b8da-b0021ce308e9"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "comments.columns = comments.columns.str.replace(' ','_')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comments['text'] = comments['Comments']"
      ],
      "metadata": {
        "id": "SrL2wsXSlQdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_Preprocess_youtube(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        #self.targets = OneHotEncoder(sparse=False).fit_transform(np.array(self.data[\"hatespeech\"]).reshape(-1, 1))\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            #\"targets\": torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "Z8DUQibFl781"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = Dataset_Preprocess_youtube(comments, TOKENIZER, MAX_LEN)"
      ],
      "metadata": {
        "id": "8FZi33ITiwat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, loader):\n",
        "    model.eval()\n",
        "    fin_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(loader, 0)):\n",
        "            ids = data[\"ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"mask\"].to(device, dtype=torch.long)\n",
        "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs"
      ],
      "metadata": {
        "id": "6wYnBHVQmMuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test, **val_params)\n",
        "predictions = prediction(model, test_loader)"
      ],
      "metadata": {
        "id": "0Qr-Sv1JiVvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result = pd.DataFrame()\n",
        "final_predictions = np.argmax(predictions, axis=1)\n",
        "final_result['comments'] = comments['Comments']\n",
        "final_result['predictions'] = final_predictions"
      ],
      "metadata": {
        "id": "YHIlW7QgycFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result.tail(40)"
      ],
      "metadata": {
        "id": "5jZ0lYd7pPDy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERT_Base_Modelo1_Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:571]",
      "language": "python",
      "name": "conda-env-571-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}